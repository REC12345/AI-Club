# -*- coding: utf-8 -*-
"""AI Competition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tqhTvCZLSxfOWzCgctBoQ0ZQ2uExTuGw
"""

# X-TRAIN data https://drive.google.com/drive/folders/17V9EmSBujRzE0ROT2qADMBCET-zdjb1X

# Spreadsheet https://docs.google.com/spreadsheets/d/1MyLhLv8aefrVIDllyxqnah2dGX29ykGEWVBpZ871czM/edit?usp=sharing

# --------------------------------------------------------------------------------------------------------

# ORIGINAL X_TRAIN CSV CANNOT BE LOADED BECAUSE IT STARTS WITH A COMMA - COMMA MUST BE REMOVED TO WORK !!!

# Row 64 clearly has misinputed data so is discarded in my  model (acceptance rate is ~8,000)

# --------------------------------------------------------------------------------------------------------

from matplotlib import pyplot as plt
import pandas as pd
import pickle
import numpy as np
import glob
import tensorflow as tf
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler


df = pd.DataFrame()



for file in glob.glob('/content/X_train.csv'):
  df = pd.concat([df, pd.read_csv(file, encoding='utf-8')])



df.head()

df.info()

df.keys()

df.hist(bins=50, figsize=(20,15))
plt.show()

# Determining correlation

df.isna()

df_cleaned = df[['Unnamed: 0','ADMrate','SAT', 'AvgCost', 'CrimeRate', 'ACT', 'Enrollment', 'FBI.TotalCrime','FBI.CrimeRate', 'Major_agriculture','Major_CS','Major_Bio','Major_MathStat']]
df_cleaned  = df_cleaned.dropna(axis=0)


df_cleaned

y = df_cleaned['Unnamed: 0']
X = df_cleaned.drop(['Unnamed: 0'], axis=1)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2, random_state=0)

early_stopping = EarlyStopping(monitor='val_loss', patience=10) # prevent overfitting

# Neural Network
model = tf.keras.models.Sequential([
#    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1)
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)
model.compile(optimizer=optimizer, loss='mse')

history = model.fit(
    X_train, y_train, epochs=1000,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping]
)

plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'], '')
  plt.xlabel("Epochs")
  plt.ylabel("MSE")
  plt.legend(["Train MSE", "Test MSE"])

y_hat_train_nn = model.predict(X_train)
y_hat_test_nn = model.predict(X_test)

print("Train MSE:", mean_squared_error(y_train, y_hat_train_nn))
print("Test MSE:", mean_squared_error(y_test, y_hat_test_nn))